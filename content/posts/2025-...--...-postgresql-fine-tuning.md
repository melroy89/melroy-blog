---
title: PostgreSQL fine-tuning
author: Melroy van den Berg
type: post
date: 2025-02-18T23:26:37+01:00
toc: true
draft: true
url: /2025/postgresql-fine-tuning
featured_image: /images/2023/02/server-security.jpg
images:
  - /images/2023/02/server-security.jpg
categories:
  - Server
  - Intermediate
  - GNU/Linux OS
tags:
  - Linux
  - GNU
  - bash
---

Fine-tuning PostgreSQL for performance.

```conf
# CONNECTIONS
#------------
# - Connection Settings -
max_connections = 200

# RESOURCE USAGE
#---------------
# - Memory -
shared_buffers = 6GB
huge_pages = on
work_mem = 20MB
maintenance_work_mem = 2GB

# - Asynchronous Behavior -
maintenance_io_concurrency = 200
max_worker_processes = 14
max_parallel_workers_per_gather = 4
max_parallel_maintenance_workers = 4
max_parallel_workers = 12

# Writte-Ahead Logging (WAL)
#---------------------------
# - Settings -
synchronous_commit = off
commit_delay = 300
# - Checkpoints -
checkpoint_timeout = 30min
max_wal_size = 60GB
min_wal_size = 4GB

# QUERY TUNING
#-------------
# - Planner Cost Constants -
random_page_cost = 1.2
effective_cache_size = 24GB
```

Let's go over each parameter I just mentioned. And explain what it does and why it is important. As well as to which value to set it to.

## Connection Settings

- `max_connections`: This parameter controls the maximum number of concurrent connections to the PostgreSQL server. Increasing this value can improve performance by allowing more clients to connect simultaneously. However, it also increases the memory usage.

## Memory

- `shared_buffers`: This parameter determines the amount of memory allocated for PostgreSQL's buffer cache. Increasing this value will allow more data to be stored in memory. This will improve performance by it you will need enough RAM to store the data.
- `huge_pages`: This parameter enables the use of huge pages in the Linux kernel. Huge pages are larger memory pages that will be pinned to memory, not getting swapped in and out of RAM. With the main benefit of reducing the overhead for paging in and out is effectively eliminated. And significantly boost in performance as a result!
- `work_mem`: This parameter determines the amount of memory allocated for sorting and hashing operations. Increasing this value can improve performance by reducing the number of disk I/O operations. But just like the other settings it also increases the memory usage obviously.
- `maintenance_work_mem`: This parameter determines the amount of memory allocated for maintenance operations such as VACUUM and ANALYZE. Increasing this value can improve performance by reducing the number of disk I/O operations. And again, of course increases the memory usage.

## Asynchronous Behavior

Increasing all these values in general improve performance by allowing more parallel operations to be executed simultaneously.

- `maintenance_io_concurrency`: This parameter controls the number of background writer processes that can write to the WAL. Increasing this value can improve performance by reducing the number of disk I/O operations.
- `max_worker_processes`: This parameter controls the maximum number of worker processes that can be created by PostgreSQL.
- `max_parallel_workers_per_gather`: This parameter controls the maximum number of parallel workers that can be created by PostgreSQL for each query.
- `max_parallel_maintenance_workers`: This parameter controls the maximum number of parallel workers that can be created by PostgreSQL for maintenance operations such as VACUUM and ANALYZE.
- `max_parallel_workers`: This parameter controls the maximum number of parallel workers that can be created by PostgreSQL.

## Write-Ahead Logging (WAL)

- `synchronous_commit`: This parameter controls whether PostgreSQL commits transactions synchronously or asynchronously. If data integrity is less important to you than response times (for example, if you are running a social networking application or processing logs) you can turn this `off`, making your transaction logs asynchronous. Will the WAL might be bigger, it will be safe and will never *NOT* result in a corrupted database.
- `commit_delay`: This parameter controls the delay in milliseconds between when a transaction is committed and when the WAL is flushed to disk. Increasing reduces the number of disk I/O operations by grouping commits together in a single WALL flush.
- `checkpoint_timeout`: This parameter controls the time interval between checkpoints. Increasing this value reduces once again the number of disk I/O operations. Try to set it to a large time like 30 minutes or even longer. The only downside is that if the server crashes, it will need more time to recovery. However, setting it too small will constantly trigger flushes / writes to the WAL.
- `max_wal_size`: This parameter controls the maximum size of the WAL. Increasing this value will increase the total WAL segments WAL size. You want the WAL be triggered by the timeout (see `checkpoint_timeout` again), so be sure to increase the `max_wal_size` to a large enough value. Triggering checkpoints by the timeout gets a much more consistent WAL flush on the server.
- `min_wal_size`: This parameter controls the minimum size of the WAL.

## Query Tuning

- `random_page_cost`: This parameter controls the cost of a random page read. You really want to reduce this value close to `1` if you are using SSD storage, since SSDs can be do very fast random reads.
- `effective_cache_size`: This parameter will set the disk cache size that is available to a single query. Increasing this value will make it more likely PostgreSQL will use an index scan instead of a sequential scan. Note that this parameter by itself does *NOT* allocate any memory. We just give it a hint.


Check checkpoint statistics:

```sql
select * FROM pg_stat_checkpointer;
```


Sources:

- https://www.enterprisedb.com/blog/tuning-maxwalsize-postgresql
- https://www.postgresql.org/docs/current/runtime-config-resource.html
- https://www.postgresql.org/docs/current/runtime-config-wal.html